- The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.

- Formally, given a training sample of tweets and labels, where label ‘1’ denotes the tweet is racist/sexist and label ‘0’ denotes the tweet is not racist/sexist, your objective is to predict the labels on the given test dataset.

# 1) Importing the important libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
import string
import nltk
%matplotlib inline

pd.set_option("display.max_colwidth", 200)

# 2) Loading the dataset

from google.colab import files
files.upload()

train = pd.read_csv('train_E6oV3lV.csv')
test = pd.read_csv('test_tweets_anuFYb8.csv')
print(f'The shape of training dataset is :: {train.shape}')
print(f'The shape of test dataset is :: {test.shape}')

train.head()

test.head()

# 3) Text Description and Exploration

train.describe()

train.label.nunique()

train.label.value_counts()

train[train.label == 1].head()

train[train.label == 0].head()

- Looking at the value counts of the labels, it can be induced that the amount of hate tweets are much higher than the non-hate tweets. And this is a case of imbalanced classification, which might lead to wrong classification.

print(f'Number of missing labels = {train["label"].isnull().sum()}')
print(f'Number of missing tweets = {train["tweet"].isnull().sum()}')

train_len = train['tweet'].str.len()
test_len = test['tweet'].str.len()

plt.figure(figsize = (8,4))
plt.hist(train_len, bins = 20, label = 'Training Tweets')
plt.hist(test_len, bins = 20, label = 'Test Tweets')
plt.legend()
plt.show()

## 4) Text Pre-processing

train.columns, test.columns

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

combo_set = train.append(test, ignore_index=True) 
combo_set.shape

combo_set.dtypes

combo_set.head()

combo_set.tail()

## 4.1) Text Cleaning

def remove_pattern(text, pattern):
    r = re.findall(pattern, text)
    for i in r:
        text = re.sub(i, '', text)
    return text

## Removing Twitter Handles
combo_set['tweet_clean'] = np.vectorize(remove_pattern)(combo_set['tweet'], "@[\w]*")
combo_set.head()

## Removing anything except alphabets and #
combo_set['tweet_clean'] = combo_set['tweet_clean'].str.replace("[^a-zA-Z#]", " ") 
combo_set.head()

combo_set['tweet_clean'] = combo_set['tweet_clean'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))
combo_set.head(3)

token_tweet = combo_set['tweet_clean'].apply(lambda x: x.split()) ## Tokenizing 
token_tweet.head()

import nltk

wn = nltk.WordNetLemmatizer()
dir(wn)

def lemmatization(twt_token):
    text = [wn.lemmatize(word) for word in twt_token]
    return text

nltk.download('wordnet')

combo_set['tweet_clean'] = combo_set['tweet_clean'].apply(lambda x: lemmatization(x))

for i in range(len(token_tweet)):
    token_tweet[i] = ' '.join(token_tweet[i])    
combo_set['tweet_clean'] = token_tweet

combo_set.head()

## 5) Text visualization

all_words = ' '.join([text for text in combo_set['tweet_clean']])
from wordcloud import WordCloud
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words) 
plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

#### NON-Racist words
normal_words =' '.join([text for text in combo_set['tweet_clean'][combo_set['label'] == 0]]) 
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)
plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

#### Racist Words
normal_words =' '.join([text for text in combo_set['tweet_clean'][combo_set['label'] == 1]]) 
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)
plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

### Hashtag Importance

def hashtags(txt):
    hashtags = []
    for i in txt:
        ht = re.findall(r'#(\w+)', i)
        hashtags.append(ht)
    return hashtags

HT_normal = hashtags(combo_set['tweet_clean'][combo_set['label'] == 0])
HT_negative = hashtags(combo_set['tweet_clean'][combo_set['label'] == 1])

print(HT_normal)
print('/n')
print(HT_negative)

## Hashtag list needed to be unnested as that contained some list of lists
HT_normal = sum(HT_normal, [])
HT_negative = sum(HT_negative,[]) 

Normal And Negative tweets

## Normal Tweets

norm = nltk.FreqDist(HT_normal)
norm_data = pd.DataFrame({'Hashtag': list(norm.keys()),'Count': list(norm.values())}) 
# selecting top 20 most frequent hashtags
norm_data = norm_data.nlargest(columns="Count", n = 20)
plt.figure(figsize=(16,5))
ax = sns.barplot(data=norm_data, x= "Hashtag", y = "Count")
ax.set(ylabel = 'Count')
plt.show()

## Negative tweets

neg = nltk.FreqDist(HT_negative)
neg_data = pd.DataFrame({'Hashtag': list(neg.keys()),'Count': list(neg.values())}) 
# selecting top 20 most frequent hashtags
neg_data = neg_data.nlargest(columns="Count", n = 20)
plt.figure(figsize=(16,5))
ax = sns.barplot(data=neg_data, x= "Hashtag", y = "Count")
ax.set(ylabel = 'Count')
plt.show()

# 6) Training and Predicting

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vect = TfidfVectorizer(stop_words='english')
tfidf = tfidf_vect.fit_transform(combo_set['tweet_clean'])

tfidf.shape

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score

train_tfidf = tfidf[:31962,:]
test_tfidf = tfidf[31962:,:]

xtrain_tfidf, xvalid_tfidf, ytrain, yvalid = train_test_split(train_tfidf, train['label'], random_state=42, test_size=0.3)
xtrain_tfidf = train_tfidf[ytrain.index]
xvalid_tfidf = train_tfidf[yvalid.index]

### 6.1) Logistic Regression

lreg = LogisticRegression()

lreg.fit(xtrain_tfidf, ytrain) 
prediction = lreg.predict_proba(xvalid_tfidf) 
prediction_int = prediction[:,1] >= 0.3
prediction_int = prediction_int.astype(np.int)
f1_score(yvalid, prediction_int)

from sklearn import svm

svc = svm.SVC(kernel='linear', 
C=1, probability=True).fit(xtrain_tfidf, ytrain) 
predict_svc = svc.predict_proba(xvalid_tfidf) 
predict_int = predict_svc[:,1] >= 0.3 
predict_int = predict_int.astype(np.int) 
f1_score(yvalid, predict_int)

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_tfidf, ytrain) 
prediction = rf.predict(xvalid_tfidf)

f1_score(yvalid, prediction)

from xgboost import XGBClassifier

xgb = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_tfidf, ytrain) 
prediction = xgb.predict(xvalid_tfidf)
f1_score(yvalid, prediction)

### Prediction of test set using SVM

test_pred = svc.predict_proba(test_tfidf)
test_pred_int = test_pred[:,1] >= 0.3
test_pred_int = test_pred_int.astype(np.int)
test['label'] = test_pred_int
submission = test[['id','label']]
submission.to_csv('sub_svc.csv', index=False)

